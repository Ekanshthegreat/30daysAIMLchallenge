![Screenshot](https://github.com/Ekanshthegreat/30daysAIMLchallenge/raw/main/Screenshots/day1ibm.png)

watched an IBM lecture on AI ML 

![Screenshot](https://github.com/Ekanshthegreat/30daysAIMLchallenge/raw/main/Screenshots/day1aws.png)

learnt what AGI is: 

Deep Learning: an AI discipline that focuses on training neural networks with multiple hidden layers to extract and understand complex relationships from raw data


Generative artificial intelligence: is a subset of deep learning wherein an AI system can produce unique and realistic content from learned knowledge
its trained with massive data sets.

Natural language processing (NLP): is a branch of AI that allows computer systems to understand and generate human language. language data into simple representations called tokens and understand their contextual relationship

Computer vision:  technology that allows systems to extract, analyze, and comprehend spatial information from visual data. Self-driving cars use computer vision models to analyze real-time feeds from cameras and navigate the vehicle safely away from obstacles. Deep learning technologies allow computer vision systems to automate large-scale object recognition, classification, monitoring, and other image-processing tasks.


![Screenshot](https://github.com/Ekanshthegreat/30daysAIMLchallenge/raw/main/Screenshots/day1ibm2.png)

GPT: generative Pre Trained Transformed is LargeLanguageModel

Learnt some fun stuff about the history of LLM's
starting with ELIZA model...didnt have much progress until google released in 2017 a research paper on Transformers architecture which allowed GPT 1 to be built with high number of parameters


![Screenshot](https://github.com/Ekanshthegreat/30daysAIMLchallenge/raw/main/Screenshots/llms.png)


How do LLM's work: Tokenization -> then vector embeddings -> then placed in vector databases (which are optimized for retrieval of vectors)
so now they can see which words are related to other words based on their embeddings. 
-> Transformations


data pre processing before feeding in and training. nvidia building hardware for these

RLHF - reinforcement learning human feedback
